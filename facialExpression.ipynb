{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affective Computing - Programming Assignment 3\n",
    "\n",
    "### Objective\n",
    "\n",
    "Your task is to use the feature-level method to combine the facial expression features and audio features. A multi-modal emotion recognition system is constructed to recognize happy versus sadness facial expressions (binary-class problem) by using a classifier training and testing structure.\n",
    "\n",
    "The original data is based on lab1 and lab2, from ten actors acting happy and sadness behaviors. \n",
    "* Task 1: Subspace-based feature fusion method: In this case, z-score normalization is utilized. Please read “Fusing Gabor and LBP feature sets for kernel-based face recognition” and learn how to use subspace-based feature fusion method for multi-modal system.\n",
    "\n",
    "* Task 2: Based on Task1, use Canonical Correlation Analysis to calculate the correlation coefficient of facial expression and audio features. Finally, use CCA to build a multi-modal emotion recognition system. The method is described in one conference paper “Feature fusion method based on canonical correlation analysis and handwritten character recognition”\n",
    "* Optional task: Use feature-level method (Task 2) on 10-fold cross-validation estimate of the emotion recognition system performance\n",
    "\n",
    "To produce emotion recognition case, Support Vector Machine (SVM) classifiers are trained.  50 videos from 5 participants are used to train the emotion recognition, use spatiotemporal features. The rest of the data (50 videos) is used to evaluate the performances of the trained recognition systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Subspace-based method\n",
    "Please read “Fusing Gabor and LBP feature sets for kernel-based face recognition” and apply their framework for the exercise. We use Support Vector Machine (SVM) with linear kernel for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment \n",
    "\n",
    "First, we need to import the basic modules for loading the data and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import dlib\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "from skimage import color\n",
    "from skimage import img_as_ubyte\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import sklearn\n",
    "import scipy.io as sio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data \n",
    "\n",
    "We load the facial expression data (training data, training class, testing data, testing class) and audio data (training data, testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata = sio.loadmat('lab3_data.mat')\n",
    "#print mdata\n",
    "\n",
    "#facial expression training and testing data, training and testing class\n",
    "training_data = mdata['training_data']\n",
    "testing_data =  mdata['testing_data']\n",
    "training_class =  mdata['training_class']\n",
    "testing_class =  mdata['testing_class']\n",
    "\n",
    "#audio training and testing data\n",
    "training_data_proso =  mdata['training_data_proso']\n",
    "testing_data_proso =  mdata['testing_data_proso']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the subspace for facial expression feature and audio features. \n",
    "Extract the subspace for facial expression feature and audio features using principal component analysis through using __[`sklearn.decomposition.PCA()`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)__ function.\n",
    "ReducedDim is the dimensionality of the reduced subspace.\n",
    "Set ReducedDim to 20 and 15 for facial expression feature and audio feature, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "\n",
    "#set ReducedDim for facial expression feature and audio feature, respectively.\n",
    "reducedDim_v = 20;\n",
    "reducedDim_a = 15;\n",
    "\n",
    "#Extract subspace for facial expression feature though PCA\n",
    "#set n_components\n",
    "pca_v=PCA(n_components=reducedDim_v, whiten=True)\n",
    "pca_v.fit(training_data)\n",
    "\n",
    "#Transform training_data and testing data respectively\n",
    "pca_face_train =pca_v.transform(training_data)\n",
    "pca_face_test =pca_v.transform(testing_data) \n",
    "\n",
    "print len(pca_face_test[0])\n",
    "print len(pca_face_train[0])\n",
    "\n",
    "#Extract subspace for audio features though PCA\n",
    "pca_a=PCA(n_components=reducedDim_a, whiten=True)\n",
    "pca_a.fit(training_data_proso)\n",
    "\n",
    "#Transform training_data and testing data respectively\n",
    "pca_audio_train  =pca_a.transform(training_data_proso)\n",
    "pca_audio_test  =pca_a.transform(testing_data_proso)  \n",
    "\n",
    "\n",
    "#Concatenate ‘video training_data’ and ‘audio training_data’ into a new feature ‘combined_trainingData’\n",
    "sample_train = np.concatenate((pca_face_train, pca_audio_train),axis =1)\n",
    "\n",
    "#Concatenate ‘video testing_data’ and ‘audio testing_data2 into a new feature ‘combined_testingData’.\n",
    "sample_test = np.concatenate((pca_face_test, pca_audio_test),axis =1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature classification\n",
    "Use the __[`SVM`](http://scikit-learn.org/stable/modules/svm.html)__ function to train Support Vector Machine (SVM) classifiers.\n",
    "Construct an SVM using the ‘combined_trainingData’ and linear kernel. The ‘training_class’ group vector contains the class of samples: 1 = happy, 2 = sadness, corresponding to the rows of the training data matrices.\n",
    "\n",
    "Then, calculate average classification performances for both training and testing data. The correct class labels corresponding with the rows of the training and testing data matrices are in the variables ‘training_class’ and ‘testing_class’, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Train SVM classifier\n",
    "clf = svm.SVC(kernel  = 'linear', cache_size = 5000)\n",
    "clf.fit(sample_train, training_class)  \n",
    "\n",
    "#The prediction results of training data and testing data respectively\n",
    "prediction_train = clf.predict(sample_train)\n",
    "prediction = clf.predict(sample_test)\n",
    "\n",
    "#Calculate and Print the training accuracy and testing accuracy. \n",
    "correct_num = 0.\n",
    "for i in range(len(training_class)):\n",
    "    if prediction_train[i]==training_class[i]:\n",
    "        correct_num+=1   \n",
    "Acc_train = correct_num/len(training_class)\n",
    "print Acc_train\n",
    "\n",
    "#    2.2 calculate the accuracy when classifying the test data\n",
    "correct_num = 0.\n",
    "for i in range(len(testing_class)):\n",
    "    if prediction[i]==testing_class[i]:\n",
    "        correct_num+=1   \n",
    "Acc_test = correct_num/len(testing_class)\n",
    "print Acc_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the confusion matrix through __[`sklearn.metrics.confusion_matrix()`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)__function for training data and testing data respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25  0]\n",
      " [ 0 25]]\n",
      "[[25  0]\n",
      " [ 1 24]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_train = confusion_matrix(training_class, prediction_train)\n",
    "print confusion_train\n",
    "\n",
    "confusion_test = confusion_matrix(testing_class, prediction)\n",
    "print confusion_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. \n",
    "Based on Task1, use Canonical Correlation Analysis to calculate the correlation coefficient of facial expression and audio features. Finally, use CCA to build a multi-modal emotion recognition system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use (__[`sklearn.cross_decomposition.CCA()`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html)__) function to calculate the correlation coefficient of facial expression and audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "import numpy as np\n",
    "\n",
    "#Use CCA to construct the Canonical Projective Vector (CPV)\n",
    "cca = CCA(15)\n",
    "cca.fit(training_data, training_data_proso)\n",
    "\n",
    "#Construct Canonical Correlation Discriminant Features (CCDF) for training data and testing data\n",
    "cca_face_train, cca_audio_train  = cca.transform(training_data, training_data_proso)\n",
    "cca_face_test, cca_audio_test  = cca.transform(testing_data, testing_data_proso)\n",
    "\n",
    "\n",
    "# Concatenate multiple feature for training data and testing data respectively\n",
    "training_CCDF = np.concatenate((cca_face_train, cca_audio_train),axis =1)\n",
    "testing_CCDF = np.concatenate((cca_face_test, cca_audio_test),axis =1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train SVM classifiers through  'linear' kernel, print the training and testing accuracy and compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.82\n",
      "[[25  0]\n",
      " [ 0 25]]\n",
      "[[25  0]\n",
      " [ 9 16]]\n"
     ]
    }
   ],
   "source": [
    "#Train svm classifier \n",
    "clf = svm.SVC(kernel  = 'linear', cache_size = 5000)\n",
    "clf.fit(training_CCDF, training_class)  \n",
    "\n",
    "#The prediction results of training data and testing data respectively\n",
    "prediction_train = clf.predict(training_CCDF)\n",
    "prediction = clf.predict(testing_CCDF)\n",
    "\n",
    "#Calculate and Print the training accuracy and testing accuracy. \n",
    "correct_num = 0.\n",
    "for i in range(len(training_class)):\n",
    "    if prediction_train[i]==training_class[i]:\n",
    "        correct_num+=1   \n",
    "Acc_train = correct_num/len(training_class)\n",
    "print Acc_train\n",
    "\n",
    "#    2.2 calculate the accuracy when classifying the test data\n",
    "correct_num = 0.\n",
    "for i in range(len(testing_class)):\n",
    "    if prediction[i]==testing_class[i]:\n",
    "        correct_num+=1   \n",
    "Acc_test = correct_num/len(testing_class)\n",
    "print Acc_test\n",
    "\n",
    "confusion_train = confusion_matrix(training_class, prediction_train)\n",
    "print confusion_train\n",
    "\n",
    "confusion_test = confusion_matrix(testing_class, prediction)\n",
    "print confusion_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional task: \n",
    "Use feature-level method (Task 2) on 10-fold cross-validation estimate of the emotion recognition system performance\n",
    "* Join the training/testing data matrices and the class vectors. Combine also the ‘training_data_personID’ and ‘testing_data_personID’ vectors that are needed to make the CV folds.\n",
    "* Construct the CV folds by training ten SVMs. For each SVM nine persons’ data is used as the training set (i.e. 90 samples) and one persons’ samples are kept as the test set (i.e. 10 samples) for the respective fold (i.e. each SVM has different persons’ samples excluded from the training set). Test each ten trained SVMs by using the corresponding one held-out persons’ samples and then calculate the average classification performances for each fold.\n",
    "* Calculate the mean and SD of the ten CV fold performances to produce the final CV performance estimate of the emotion recognition system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
